Here's a `README.md` file for the "webcrawlers" GitHub repository:

```markdown
# Web Crawlers

This repository contains a Python script called `webcrawler.py` that implements a web crawler to find all links on a web page. The crawler creates two lists: "crawled" and "not crawled" to track the visited and unvisited links, respectively.

## Prerequisites

To run the web crawler script, you need to have the following dependencies installed:

- Python 3.x
- `requests` library
- `beautifulsoup4` library

You can install the required libraries using pip:

```bash
pip install requests beautifulsoup4
```

## Usage

1. Clone the repository to your local machine:

```bash
git clone https://github.com/TheRealSaiTama/webcrawlers.git
```

2. Navigate to the repository directory:

```bash
cd webcrawlers
```

3. Run the `webcrawler.py` script:

```bash
python webcrawler.py
```

The script will crawl the web page specified in the `url` variable and print out all the links found.

## License

This project is licensed under the [MIT License](LICENSE).

Feel free to explore, modify, and use the code in accordance with the license terms.

```
Have Fun
